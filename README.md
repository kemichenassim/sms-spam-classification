# ğŸ“± Classification de SMS Spam  
### PrÃ©traitement Textuel & Apprentissage Profond  
**Ã‰valuation Sommative UA3 â€“ Intelligence Artificielle**

---

## ğŸ¯ Objectif du projet

Ce projet vise Ã  dÃ©velopper et comparer plusieurs approches de classification de SMS (Ham vs Spam) en utilisant :

- ğŸ“ PrÃ©traitement textuel avancÃ©  
- ğŸ”¢ Vectorisation TF-IDF  
- ğŸ”„ Autoencodeur (rÃ©duction non-supervisÃ©e)  
- ğŸ¤– BERT (Transformers â€“ embeddings contextuels)

Lâ€™objectif principal est dâ€™identifier la meilleure approche dans un contexte de **dataset dÃ©sÃ©quilibrÃ©**.

---

## ğŸ“Š Dataset

- **Nom** : SMS Spam Collection  
- **Source** : UCI Machine Learning Repository  
- **Nombre total de messages** : 5 572  

### Distribution des classes

| Classe | Nombre | Pourcentage |
|--------|--------|------------|
| Ham (LÃ©gitime) | 4 825 | 86.6% |
| Spam | 747 | 13.4% |

âš ï¸ Le dataset est fortement dÃ©sÃ©quilibrÃ©.  
Lâ€™accuracy seule nâ€™est donc pas une mÃ©trique fiable.

---

## ğŸ” Analyse Exploratoire (EDA)

- Aucune valeur manquante  
- 5 169 messages uniques  
- Les messages SPAM sont en moyenne **deux fois plus longs** que les messages HAM  
- La longueur du message constitue une caractÃ©ristique discriminante importante  

---

## ğŸ”§ Pipeline de PrÃ©traitement

1. Conversion en minuscules  
2. Suppression de la ponctuation et caractÃ¨res spÃ©ciaux  
3. Tokenisation  
4. Suppression des stopwords  
5. Lemmatisation  


## ğŸ“ Approches ComparÃ©es

### 1ï¸âƒ£ TF-IDF + RÃ©seau Dense

- max_features = 5000  
- ngram_range = (1,2)  
- RÃ©seau dense (128 â†’ 64 â†’ 32 â†’ 1)  

**RÃ©sultats :**

- Accuracy : 97.58%  
- AUC-ROC : 0.9828  
- Recall Spam : 87%  

âœ… Excellent compromis performance / simplicitÃ©  

---

### 2ï¸âƒ£ Autoencodeur (RÃ©duction non-supervisÃ©e)

Compression : 5000 â†’ 64 dimensions  

**RÃ©sultats :**

- Accuracy : 86.64%  
- AUC-ROC : 0.4127  
- Recall Spam : 0%  

âŒ Ã‰chec critique  

Le modÃ¨le prÃ©dit tout comme "Ham".  
Lâ€™accuracy est trompeuse dans un dataset dÃ©sÃ©quilibrÃ©.

ğŸ“Œ LeÃ§on fondamentale :  
Lâ€™accuracy seule peut Ãªtre dangereusement trompeuse.

---

### 3ï¸âƒ£ BERT (Transformers)

- ModÃ¨le : bert-base-uncased  
- 110 millions de paramÃ¨tres  
- Embeddings de 768 dimensions  

**RÃ©sultats :**

- Accuracy : 98.25%  
- AUC-ROC : 0.9974  
- Recall Spam : 91%  

ğŸ† Meilleure performance globale  

---

## ğŸ† Classement Final

| Rang | ModÃ¨le | Accuracy | AUC |
|------|--------|----------|-----|
| ğŸ¥‡ | BERT | 98.25% | 0.9974 |
| ğŸ¥ˆ | TF-IDF | 97.58% | 0.9828 |
| ğŸ¥‰ | Autoencodeur | 86.64% | 0.4127 |

---

## ğŸ“ˆ Enseignements ClÃ©s

- Lâ€™accuracy est insuffisante pour les datasets dÃ©sÃ©quilibrÃ©s  
- Toujours analyser Recall et AUC-ROC  
- La rÃ©duction non-supervisÃ©e peut perdre lâ€™information discriminante  
- BERT capture la sÃ©mantique contextuelle et amÃ©liore significativement la performance  

---

## ğŸš€ AmÃ©liorations Futures

### Court terme
- Fine-tuning de BERT  
- PondÃ©ration des classes (class_weight)  
- Ã‰quilibrage via SMOTE  

### Long terme
- Ensemble de modÃ¨les  
- Autoencodeur supervisÃ©  
- Optimisation des hyperparamÃ¨tres (GridSearch)  
- DÃ©ploiement en production  

---

## ğŸ’¡ StratÃ©gie Hybride ProposÃ©e

Utiliser :

- TF-IDF pour filtrage rapide (majoritÃ© des cas)  
- BERT pour les cas ambigus  

Optimisation du ratio coÃ»t / performance.

---

## ğŸ› ï¸ Technologies UtilisÃ©es

- Python  
- Scikit-learn  
- TensorFlow / Keras  
- HuggingFace Transformers  
- NLTK / SpaCy  
- Matplotlib / Seaborn  

---

## ğŸ‘¨â€ğŸ“ Auteur

**Kemiche Nassim**  
Ã‰tudiant en Science des DonnÃ©es  
CollÃ¨ge La CitÃ©  

GitHub : https://github.com/kemichenassim

